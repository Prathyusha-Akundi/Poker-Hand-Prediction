{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import kornia\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scratch/prathyuakundi/aicrowd/poker/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = root_dir+\"train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>C1</th>\n",
       "      <th>S2</th>\n",
       "      <th>C2</th>\n",
       "      <th>S3</th>\n",
       "      <th>C3</th>\n",
       "      <th>S4</th>\n",
       "      <th>C4</th>\n",
       "      <th>S5</th>\n",
       "      <th>C5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S1  C1  S2  C2  S3  C3  S4  C4  S5  C5  label\n",
       "0   1   1   1  13   2   4   2   3   1  12      0\n",
       "1   3  12   3   2   3  11   4   5   2   5      1\n",
       "2   1   9   4   6   1   4   3   2   3   9      1\n",
       "3   1   4   3  13   2  13   2   1   3   6      1\n",
       "4   3  10   2   7   1   2   2  11   4   9      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val= train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train,y_train = X_train[:,:-1],X_train[:,-1]\n",
    "X_val,y_val = X_val[:,:-1],X_val[:,-1]\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerData(Dataset):\n",
    "    def __init__(self,cards,hand = None,transform=None,train=True):\n",
    "        super().__init__()\n",
    "        self.cards = cards\n",
    "        self.hand = hand\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.cards.shape[0]\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        \n",
    "        if self.train:\n",
    "            target = self.hand[item]\n",
    "        \n",
    "        suit_cards = self.cards[item]\n",
    "        \n",
    "        if self.train:\n",
    "          return {\n",
    "              'cards' : suit_cards,\n",
    "              'hand' : torch.tensor(target)\n",
    "\n",
    "          }\n",
    "        else:\n",
    "          return {\n",
    "              'cards':suit_cards\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PokerData(X_train, hand = y_train, train=True)\n",
    "\n",
    "valid_data = PokerData(X_val, hand = y_val, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 5, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = batch)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 1024\n",
    "        hidden_2 = 512\n",
    "        hidden_3 = 256\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_2)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_3)\n",
    "        # linear layer (10 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(10, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_3, num_classes)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 10)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=10, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True, factor = 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/scratch/prathyuakundi/aicrowd/poker/model.pth'\n",
    "loaders = {\"train\" : train_loader, \"valid\" : valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.997954 \tValidation Loss: 0.865497\n",
      "Validation loss decreased (inf --> 0.865497).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.846299 \tValidation Loss: 0.771711\n",
      "Validation loss decreased (0.865497 --> 0.771711).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.708631 \tValidation Loss: 0.408314\n",
      "Validation loss decreased (0.771711 --> 0.408314).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.324866 \tValidation Loss: 0.052914\n",
      "Validation loss decreased (0.408314 --> 0.052914).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.107718 \tValidation Loss: 0.011291\n",
      "Validation loss decreased (0.052914 --> 0.011291).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.052582 \tValidation Loss: 0.005266\n",
      "Validation loss decreased (0.011291 --> 0.005266).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.033744 \tValidation Loss: 0.002995\n",
      "Validation loss decreased (0.005266 --> 0.002995).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.024918 \tValidation Loss: 0.002000\n",
      "Validation loss decreased (0.002995 --> 0.002000).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.019867 \tValidation Loss: 0.001589\n",
      "Validation loss decreased (0.002000 --> 0.001589).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.015448 \tValidation Loss: 0.000992\n",
      "Validation loss decreased (0.001589 --> 0.000992).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.013584 \tValidation Loss: 0.001031\n",
      "Validation loss not improved from  0.0009916314625740051\n",
      "Epoch: 12 \tTraining Loss: 0.011496 \tValidation Loss: 0.000921\n",
      "Validation loss decreased (0.000992 --> 0.000921).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.010249 \tValidation Loss: 0.000778\n",
      "Validation loss decreased (0.000921 --> 0.000778).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.009436 \tValidation Loss: 0.000649\n",
      "Validation loss decreased (0.000778 --> 0.000649).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.008554 \tValidation Loss: 0.000663\n",
      "Validation loss not improved from  0.000648930249218829\n",
      "Epoch: 16 \tTraining Loss: 0.007548 \tValidation Loss: 0.000650\n",
      "Validation loss not improved from  0.000648930249218829\n",
      "Epoch: 17 \tTraining Loss: 0.006788 \tValidation Loss: 0.000537\n",
      "Validation loss decreased (0.000649 --> 0.000537).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.007229 \tValidation Loss: 0.000462\n",
      "Validation loss decreased (0.000537 --> 0.000462).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.006147 \tValidation Loss: 0.000609\n",
      "Validation loss not improved from  0.0004615722131705843\n",
      "Epoch: 20 \tTraining Loss: 0.005747 \tValidation Loss: 0.000483\n",
      "Validation loss not improved from  0.0004615722131705843\n",
      "Epoch: 21 \tTraining Loss: 0.005916 \tValidation Loss: 0.000396\n",
      "Validation loss decreased (0.000462 --> 0.000396).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.005321 \tValidation Loss: 0.000431\n",
      "Validation loss not improved from  0.00039632854700204917\n",
      "Epoch: 23 \tTraining Loss: 0.005043 \tValidation Loss: 0.000386\n",
      "Validation loss decreased (0.000396 --> 0.000386).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.005243 \tValidation Loss: 0.000779\n",
      "Validation loss not improved from  0.0003860654187202454\n",
      "Epoch: 25 \tTraining Loss: 0.005139 \tValidation Loss: 0.000404\n",
      "Validation loss not improved from  0.0003860654187202454\n",
      "Epoch: 26 \tTraining Loss: 0.004473 \tValidation Loss: 0.001706\n",
      "Validation loss not improved from  0.0003860654187202454\n",
      "Epoch: 27 \tTraining Loss: 0.004253 \tValidation Loss: 0.000539\n",
      "Validation loss not improved from  0.0003860654187202454\n",
      "Epoch: 28 \tTraining Loss: 0.004374 \tValidation Loss: 0.000331\n",
      "Validation loss decreased (0.000386 --> 0.000331).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.004599 \tValidation Loss: 0.000391\n",
      "Validation loss not improved from  0.00033122104406473226\n",
      "Epoch: 30 \tTraining Loss: 0.003817 \tValidation Loss: 0.000449\n",
      "Validation loss not improved from  0.00033122104406473226\n",
      "Epoch: 31 \tTraining Loss: 0.004140 \tValidation Loss: 0.000606\n",
      "Validation loss not improved from  0.00033122104406473226\n",
      "Epoch: 32 \tTraining Loss: 0.003698 \tValidation Loss: 0.000302\n",
      "Validation loss decreased (0.000331 --> 0.000302).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.003781 \tValidation Loss: 0.000423\n",
      "Validation loss not improved from  0.00030215501546743324\n",
      "Epoch: 34 \tTraining Loss: 0.003547 \tValidation Loss: 0.000262\n",
      "Validation loss decreased (0.000302 --> 0.000262).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.003392 \tValidation Loss: 0.000587\n",
      "Validation loss not improved from  0.00026199283122783527\n",
      "Epoch: 36 \tTraining Loss: 0.003481 \tValidation Loss: 0.000385\n",
      "Validation loss not improved from  0.00026199283122783527\n",
      "Epoch: 37 \tTraining Loss: 0.003191 \tValidation Loss: 0.000390\n",
      "Validation loss not improved from  0.00026199283122783527\n",
      "Epoch: 38 \tTraining Loss: 0.003574 \tValidation Loss: 0.000244\n",
      "Validation loss decreased (0.000262 --> 0.000244).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.003214 \tValidation Loss: 0.000204\n",
      "Validation loss decreased (0.000244 --> 0.000204).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.003272 \tValidation Loss: 0.000252\n",
      "Validation loss not improved from  0.00020357459783321247\n",
      "Epoch: 41 \tTraining Loss: 0.002755 \tValidation Loss: 0.000269\n",
      "Validation loss not improved from  0.00020357459783321247\n",
      "Epoch: 42 \tTraining Loss: 0.002932 \tValidation Loss: 0.000253\n",
      "Validation loss not improved from  0.00020357459783321247\n",
      "Epoch: 43 \tTraining Loss: 0.002956 \tValidation Loss: 0.000195\n",
      "Validation loss decreased (0.000204 --> 0.000195).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.003197 \tValidation Loss: 0.000232\n",
      "Validation loss not improved from  0.00019472748994710855\n",
      "Epoch: 45 \tTraining Loss: 0.002883 \tValidation Loss: 0.000173\n",
      "Validation loss decreased (0.000195 --> 0.000173).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.002678 \tValidation Loss: 0.000154\n",
      "Validation loss decreased (0.000173 --> 0.000154).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.002930 \tValidation Loss: 0.000256\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch: 48 \tTraining Loss: 0.002580 \tValidation Loss: 0.000277\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch: 49 \tTraining Loss: 0.002299 \tValidation Loss: 0.000236\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch: 50 \tTraining Loss: 0.002703 \tValidation Loss: 0.000216\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch: 51 \tTraining Loss: 0.002202 \tValidation Loss: 0.000177\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch    52: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch: 52 \tTraining Loss: 0.002630 \tValidation Loss: 0.000235\n",
      "Validation loss not improved from  0.00015418089151615278\n",
      "Epoch: 53 \tTraining Loss: 0.001700 \tValidation Loss: 0.000100\n",
      "Validation loss decreased (0.000154 --> 0.000100).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.001513 \tValidation Loss: 0.000132\n",
      "Validation loss not improved from  9.993520736694336e-05\n",
      "Epoch: 55 \tTraining Loss: 0.001679 \tValidation Loss: 0.000173\n",
      "Validation loss not improved from  9.993520736694336e-05\n",
      "Epoch: 56 \tTraining Loss: 0.002068 \tValidation Loss: 0.000095\n",
      "Validation loss decreased (0.000100 --> 0.000095).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.001405 \tValidation Loss: 0.000116\n",
      "Validation loss not improved from  9.476513862668071e-05\n",
      "Epoch: 58 \tTraining Loss: 0.001564 \tValidation Loss: 0.000080\n",
      "Validation loss decreased (0.000095 --> 0.000080).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.001542 \tValidation Loss: 0.000187\n",
      "Validation loss not improved from  8.025669813039713e-05\n",
      "Epoch: 60 \tTraining Loss: 0.001737 \tValidation Loss: 0.000086\n",
      "Validation loss not improved from  8.025669813039713e-05\n",
      "Epoch: 61 \tTraining Loss: 0.001510 \tValidation Loss: 0.000097\n",
      "Validation loss not improved from  8.025669813039713e-05\n",
      "Epoch: 62 \tTraining Loss: 0.001691 \tValidation Loss: 0.000146\n",
      "Validation loss not improved from  8.025669813039713e-05\n",
      "Epoch: 63 \tTraining Loss: 0.001618 \tValidation Loss: 0.000204\n",
      "Validation loss not improved from  8.025669813039713e-05\n",
      "Epoch: 64 \tTraining Loss: 0.001265 \tValidation Loss: 0.000074\n",
      "Validation loss decreased (0.000080 --> 0.000074).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.001588 \tValidation Loss: 0.000078\n",
      "Validation loss not improved from  7.356461048126221e-05\n",
      "Epoch: 66 \tTraining Loss: 0.001450 \tValidation Loss: 0.000084\n",
      "Validation loss not improved from  7.356461048126221e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 \tTraining Loss: 0.001460 \tValidation Loss: 0.000187\n",
      "Validation loss not improved from  7.356461048126221e-05\n",
      "Epoch: 68 \tTraining Loss: 0.001320 \tValidation Loss: 0.000059\n",
      "Validation loss decreased (0.000074 --> 0.000059).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.001670 \tValidation Loss: 0.000097\n",
      "Validation loss not improved from  5.8727869987778834e-05\n",
      "Epoch: 70 \tTraining Loss: 0.001274 \tValidation Loss: 0.000131\n",
      "Validation loss not improved from  5.8727869987778834e-05\n",
      "Epoch: 71 \tTraining Loss: 0.001463 \tValidation Loss: 0.000114\n",
      "Validation loss not improved from  5.8727869987778834e-05\n",
      "Epoch: 72 \tTraining Loss: 0.001383 \tValidation Loss: 0.000172\n",
      "Validation loss not improved from  5.8727869987778834e-05\n",
      "Epoch: 73 \tTraining Loss: 0.001113 \tValidation Loss: 0.000038\n",
      "Validation loss decreased (0.000059 --> 0.000038).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.001187 \tValidation Loss: 0.000055\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch: 75 \tTraining Loss: 0.001260 \tValidation Loss: 0.000115\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch: 76 \tTraining Loss: 0.001404 \tValidation Loss: 0.000038\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch: 77 \tTraining Loss: 0.001115 \tValidation Loss: 0.000038\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch: 78 \tTraining Loss: 0.001293 \tValidation Loss: 0.000042\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 3.2000e-04.\n",
      "Epoch: 79 \tTraining Loss: 0.001337 \tValidation Loss: 0.000110\n",
      "Validation loss not improved from  3.7575912474421785e-05\n",
      "Epoch: 80 \tTraining Loss: 0.000815 \tValidation Loss: 0.000017\n",
      "Validation loss decreased (0.000038 --> 0.000017).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.000884 \tValidation Loss: 0.000031\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 82 \tTraining Loss: 0.000949 \tValidation Loss: 0.000029\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 83 \tTraining Loss: 0.000951 \tValidation Loss: 0.000041\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 84 \tTraining Loss: 0.000864 \tValidation Loss: 0.000037\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 85 \tTraining Loss: 0.000807 \tValidation Loss: 0.000027\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch    86: reducing learning rate of group 0 to 2.5600e-04.\n",
      "Epoch: 86 \tTraining Loss: 0.000903 \tValidation Loss: 0.000022\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 87 \tTraining Loss: 0.000666 \tValidation Loss: 0.000042\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 88 \tTraining Loss: 0.000674 \tValidation Loss: 0.000051\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 89 \tTraining Loss: 0.000680 \tValidation Loss: 0.000109\n",
      "Validation loss not improved from  1.7142791747464797e-05\n",
      "Epoch: 90 \tTraining Loss: 0.000554 \tValidation Loss: 0.000021\n",
      "Validation loss not improved from  1.7142791747464797e-05\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "patience = 0\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for dict_ in loaders['train']:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        data, target = dict_['cards'],dict_['hand']\n",
    "        data = data.float()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for dict_ in loaders['valid']:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        data, target = dict_['cards'],dict_['hand']\n",
    "        data = data.float()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    scheduler.step(valid_loss)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        valid_loss_min = valid_loss\n",
    "        patience = 0\n",
    "        \n",
    "    else:\n",
    "        print('Validation loss not improved from ', valid_loss_min)\n",
    "        patience+=1\n",
    "        \n",
    "        \n",
    "    if(patience>=10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_valid(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for dict_ in loaders['valid']:\n",
    "        data, target = dict_['cards'],dict_['hand']\n",
    "        data = data.float()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "#         test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        pred = predicted.detach().cpu().numpy()\n",
    "        for i in pred:\n",
    "            preds.append(i)\n",
    "        # compare predictions to true label\n",
    "        target_ = target.cpu().numpy()\n",
    "        correct += np.sum(np.squeeze(pred==target_))\n",
    "        total += data.size(0)\n",
    "            \n",
    "#     print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 100% (200000/200000)\n"
     ]
    }
   ],
   "source": [
    "preds = test_valid(loaders, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_path = root_dir+\"test.csv\"\n",
    "final_test = pd.read_csv(final_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = final_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = scaler.transform(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = PokerData(final_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=batch, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for dict_ in test_loader:\n",
    "        data = dict_['cards'].cuda()\n",
    "        data = data.float()\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        pred = predicted.detach().cpu().numpy()\n",
    "        for i in pred:\n",
    "            preds.append(i)\n",
    "       \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds)\n",
    "submission.to_csv('submission.csv',header=['label'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
